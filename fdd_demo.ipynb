{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# LLM-Assisted Fault Detection & Diagnosis (FDD) Demo\n\n**Goal:** Train a time-series classifier for vehicle sensor fault types (0â€“9) and generate clear technical explanations using a small open-source LLM.\n\nThis notebook is designed to run **top-to-bottom** in Google Colab.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Setup\nInstall required libraries.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!pip -q install numpy pandas scikit-learn torch torchvision torchaudio plotly gradio==4.44.0 transformers==4.44.2 accelerate bitsandbytes sentencepiece"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Imports & Config\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport json\nimport math\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport plotly.graph_objects as go\nimport gradio as gr\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nRNG = np.random.default_rng(42)\n\nDATA_PATH = \"/content\"  # upload CSV here in Colab\nCSV_FILE = None  # set to filename after upload (e.g., \"vehicle_faults.csv\")\n\nFEATURE_COLS = [\n    \"ENGINE_RPM\",\n    \"Trq_MeanEff_Engine_Mod[Nm]\",\n    \"p_InMan[Pa]\",\n    \"p_Rail[bar]\",\n    \"v_vehicle[km|h]\",\n]\nLABEL_COL = \"type\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Load Data\nPlace your CSV in `/content/` and set `CSV_FILE` accordingly.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# If running in Colab, upload the file via the left sidebar or: from google.colab import files; files.upload()\n\nif CSV_FILE is None:\n    candidates = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]\n    if not candidates:\n        raise FileNotFoundError(\"No CSV found in /content/. Upload your dataset and set CSV_FILE.\")\n    CSV_FILE = candidates[0]\n\ncsv_path = os.path.join(DATA_PATH, CSV_FILE)\nprint(f\"Using CSV: {csv_path}\")\n\ndf = pd.read_csv(csv_path)\n\nmissing = [c for c in FEATURE_COLS + [LABEL_COL] if c not in df.columns]\nif missing:\n    raise ValueError(f\"Missing columns: {missing}\")\n\n# Basic cleaning\ndf = df.dropna(subset=FEATURE_COLS + [LABEL_COL]).reset_index(drop=True)\n\nprint(df.head())\nprint(df[LABEL_COL].value_counts().sort_index())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Windowing + Normalization (Training Stats Only)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass WindowConfig:\n    window_size: int = 128\n    stride: int = 64\n\nwin_cfg = WindowConfig()\n\n# Split by rows first (stratified), then window within each split\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.2,\n    random_state=42,\n    stratify=df[LABEL_COL],\n)\n\n# Compute normalization stats ONLY on training data\ntrain_stats = {\n    \"mean\": train_df[FEATURE_COLS].mean().to_dict(),\n    \"std\": (train_df[FEATURE_COLS].std() + 1e-8).to_dict(),\n}\n\n# Save stats for later\nos.makedirs(\"artifacts\", exist_ok=True)\nwith open(\"artifacts/normalization_stats.json\", \"w\") as f:\n    json.dump(train_stats, f, indent=2)\n\n\ndef normalize(df_in: pd.DataFrame, stats: Dict) -> pd.DataFrame:\n    out = df_in.copy()\n    for c in FEATURE_COLS:\n        out[c] = (out[c] - stats[\"mean\"][c]) / stats[\"std\"][c]\n    return out\n\n\ndef window_data(df_in: pd.DataFrame, win_cfg: WindowConfig) -> Tuple[np.ndarray, np.ndarray]:\n    data = df_in[FEATURE_COLS].values\n    labels = df_in[LABEL_COL].values.astype(int)\n    X_list, y_list = [], []\n    for start in range(0, len(df_in) - win_cfg.window_size + 1, win_cfg.stride):\n        end = start + win_cfg.window_size\n        X_list.append(data[start:end])\n        # majority label in window\n        y_list.append(int(np.bincount(labels[start:end]).argmax()))\n    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.int64)\n\ntrain_df_norm = normalize(train_df, train_stats)\ntest_df_norm = normalize(test_df, train_stats)\n\nX_train, y_train = window_data(train_df_norm, win_cfg)\nX_test, y_test = window_data(test_df_norm, win_cfg)\n\nprint(\"Train windows:\", X_train.shape, \"Test windows:\", X_test.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Dataset, Model, and Class Imbalance Handling\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class WindowDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\nclass CNNGRU(nn.Module):\n    def __init__(self, num_features: int, num_classes: int = 10):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(num_features, 64, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.MaxPool1d(2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.MaxPool1d(2),\n        )\n        self.gru = nn.GRU(128, 128, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        # x: [B, T, F] -> [B, F, T]\n        x = x.transpose(1, 2)\n        x = self.conv(x)\n        # [B, C, T] -> [B, T, C]\n        x = x.transpose(1, 2)\n        out, _ = self.gru(x)\n        # use last timestep\n        return self.fc(out[:, -1, :])\n\n\ntrain_ds = WindowDataset(X_train, y_train)\ntest_ds = WindowDataset(X_test, y_test)\n\n# Handle class imbalance: class weights + weighted sampling\nclass_counts = np.bincount(y_train, minlength=10)\nclass_weights = 1.0 / np.maximum(class_counts, 1)\nclass_weights = class_weights / class_weights.sum() * len(class_weights)\n\nweights_per_sample = class_weights[y_train]\nsampler = WeightedRandomSampler(weights_per_sample, num_samples=len(weights_per_sample), replacement=True)\n\ntrain_loader = DataLoader(train_ds, batch_size=64, sampler=sampler)\ntest_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n\nmodel = CNNGRU(num_features=len(FEATURE_COLS)).to(DEVICE)\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(DEVICE))\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Train\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def train_epoch(model, loader):\n    model.train()\n    total_loss = 0.0\n    for Xb, yb in loader:\n        Xb = Xb.to(DEVICE)\n        yb = yb.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(Xb)\n        loss = criterion(logits, yb)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * Xb.size(0)\n    return total_loss / len(loader.dataset)\n\n\nEPOCHS = 5\nfor epoch in range(1, EPOCHS + 1):\n    loss = train_epoch(model, train_loader)\n    print(f\"Epoch {epoch}/{EPOCHS} - loss: {loss:.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Evaluate\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def predict(model, loader):\n    model.eval()\n    preds = []\n    labels = []\n    with torch.no_grad():\n        for Xb, yb in loader:\n            Xb = Xb.to(DEVICE)\n            logits = model(Xb)\n            pred = torch.argmax(logits, dim=1).cpu().numpy()\n            preds.append(pred)\n            labels.append(yb.numpy())\n    return np.concatenate(preds), np.concatenate(labels)\n\n\npreds, labels = predict(model, test_loader)\nprint(\"Accuracy:\", accuracy_score(labels, preds))\nprint(classification_report(labels, preds, digits=3))\n\ncm = confusion_matrix(labels, preds)\nprint(\"Confusion matrix:\\n\", cm)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Save Model & Artifacts\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "os.makedirs(\"artifacts\", exist_ok=True)\n\n# Save model\ntorch.save(model.state_dict(), \"artifacts/fdd_cnn_gru.pt\")\n\n# Save window config\nwith open(\"artifacts/window_config.json\", \"w\") as f:\n    json.dump({\"window_size\": win_cfg.window_size, \"stride\": win_cfg.stride}, f, indent=2)\n\n# Save feature list\nwith open(\"artifacts/feature_cols.json\", \"w\") as f:\n    json.dump(FEATURE_COLS, f, indent=2)\n\nprint(\"Saved artifacts to /content/artifacts\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Explainability: Knowledge Base + Anomaly Summary\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "FAULT_KB = {\n    0: {\"name\": \"healthy\", \"definition\": \"Signals are within expected ranges with normal variability and dynamics.\"},\n    1: {\"name\": \"gain\", \"definition\": \"Sensor gain error causing proportional scaling of readings.\"},\n    2: {\"name\": \"offset\", \"definition\": \"Sensor bias causing constant shift in readings.\"},\n    3: {\"name\": \"noise\", \"definition\": \"High-frequency random fluctuations dominate the signal.\"},\n    4: {\"name\": \"stuck-at\", \"definition\": \"Signal remains nearly constant regardless of system changes.\"},\n    5: {\"name\": \"drift\", \"definition\": \"Slow, continuous deviation over time from expected baseline.\"},\n    6: {\"name\": \"hard-over\", \"definition\": \"Signal saturates at a fixed high/low limit.\"},\n    7: {\"name\": \"spike\", \"definition\": \"Brief sharp excursions from normal values.\"},\n    8: {\"name\": \"delay\", \"definition\": \"Sensor response lags behind related signals.\"},\n    9: {\"name\": \"packet loss\", \"definition\": \"Intermittent missing or repeated values due to transmission loss.\"},\n}\n\n\ndef _linear_slope(y):\n    x = np.arange(len(y))\n    A = np.vstack([x, np.ones(len(y))]).T\n    m, _ = np.linalg.lstsq(A, y, rcond=None)[0]\n    return float(m)\n\n\ndef compute_anomaly_summary(window: np.ndarray) -> Dict[str, Dict[str, float]]:\n    # window shape: [T, F]\n    summary = {}\n    for i, col in enumerate(FEATURE_COLS):\n        series = window[:, i]\n        diffs = np.diff(series)\n        spikes = int(np.sum(np.abs(series - series.mean()) > 3 * series.std()))\n        flatline = float(np.std(series) < 0.01)\n        summary[col] = {\n            \"mean\": float(series.mean()),\n            \"std\": float(series.std()),\n            \"slope\": _linear_slope(series),\n            \"delta_mean\": float(diffs.mean()) if len(diffs) else 0.0,\n            \"delta_std\": float(diffs.std()) if len(diffs) else 0.0,\n            \"spike_count\": spikes,\n            \"flatline_flag\": flatline,\n        }\n    # crude delay estimate between engine RPM and vehicle speed\n    rpm = window[:, FEATURE_COLS.index(\"ENGINE_RPM\")]\n    spd = window[:, FEATURE_COLS.index(\"v_vehicle[km|h]\")]\n    corr = np.correlate(rpm - rpm.mean(), spd - spd.mean(), mode='full')\n    lag = int(np.argmax(corr) - (len(rpm) - 1))\n    summary[\"delay_estimate\"] = {\"rpm_vs_speed_lag\": lag}\n    return summary\n\n\ndef fallback_explanation(pred_class: int, confidence: float, summary: Dict[str, Dict[str, float]]) -> str:\n    fault = FAULT_KB[pred_class]\n    bullet_points = []\n    for k, v in summary.items():\n        if k == \"delay_estimate\":\n            bullet_points.append(f\"Estimated lag (RPM vs speed): {v['rpm_vs_speed_lag']} samples\")\n            continue\n        bullet_points.append(\n            f\"{k}: mean={v['mean']:.2f}, std={v['std']:.2f}, slope={v['slope']:.4f}, spikes={v['spike_count']}\"\n        )\n    bullets = \"\\n\".join([f\"- {b}\" for b in bullet_points[:6]])\n    reason = (\n        f\"Predicted class {pred_class} ({fault['name']}) with confidence {confidence:.2f}. \"\n        f\"This aligns with the definition: {fault['definition']}\"\n    )\n    checks = \"Recommended checks: verify sensor wiring, compare against redundant signals, and inspect ECU logs.\"\n    return f\"Evidence:\\n{bullets}\\n\\nReason: {reason}\\n\\n{checks}\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) LLM Explainability (Open-Source, No Paid API)\nWe use a small instruction-tuned model that can run in Colab. The prompt includes **only**: predicted class, confidence, anomaly summary, and fault definition.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\nLLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\nllm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    LLM_MODEL,\n    device_map=\"auto\",\n    load_in_4bit=True,\n)\n\n\ndef build_llm_prompt(pred_class: int, confidence: float, summary: Dict[str, Dict[str, float]]) -> str:\n    fault = FAULT_KB[pred_class]\n    payload = {\n        \"predicted_class\": pred_class,\n        \"confidence\": round(float(confidence), 4),\n        \"anomaly_summary\": summary,\n        \"fault_definition\": fault,\n    }\n    return (\n        \"You are a vehicle diagnostics assistant. Use ONLY the provided JSON. \"\n        \"Produce:\\n\"\n        \"- 3 to 6 evidence bullet points (signal-based)\\n\"\n        \"- 1 short reason paragraph\\n\"\n        \"- recommended next checks\\n\"\n        \"\\nJSON:\\n\" + json.dumps(payload, indent=2)\n    )\n\n\ndef generate_llm_explanation(pred_class: int, confidence: float, summary: Dict[str, Dict[str, float]]) -> str:\n    prompt = build_llm_prompt(pred_class, confidence, summary)\n    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n    outputs = llm_model.generate(\n        **inputs,\n        max_new_tokens=300,\n        do_sample=True,\n        temperature=0.4,\n    )\n    text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Strip prompt for readability\n    return text.split(\"JSON:\")[-1].strip()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) Inference Helpers\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def predict_window(window: np.ndarray) -> Tuple[int, float]:\n    model.eval()\n    with torch.no_grad():\n        x = torch.tensor(window, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n        logits = model(x)\n        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n    pred_class = int(np.argmax(probs))\n    confidence = float(np.max(probs))\n    return pred_class, confidence\n\n\ndef run_explain(window: np.ndarray, use_llm: bool = True) -> str:\n    pred_class, confidence = predict_window(window)\n    summary = compute_anomaly_summary(window)\n    if use_llm:\n        try:\n            return generate_llm_explanation(pred_class, confidence, summary)\n        except Exception as exc:\n            print(\"LLM failed, using fallback:\", exc)\n    return fallback_explanation(pred_class, confidence, summary)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12) Gradio Demo UI\nFeatures:\n- Upload CSV and select a window\n- Or paste manual CSV rows\n- Plotly signal visualization\n- Predicted fault + confidence\n- Explanation with LLM or fallback\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_plot(window: np.ndarray) -> go.Figure:\n    fig = go.Figure()\n    x = np.arange(window.shape[0])\n    for i, col in enumerate(FEATURE_COLS):\n        fig.add_trace(go.Scatter(x=x, y=window[:, i], mode='lines', name=col))\n    fig.update_layout(title=\"Sensor Window\", template=\"plotly_white\", height=350)\n    return fig\n\n\n# Cache CSV windows for UI\nUI_CACHE = {}\n\n\ndef load_csv_windows(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n    df_local = pd.read_csv(file_path).dropna(subset=FEATURE_COLS + [LABEL_COL])\n    df_local_norm = normalize(df_local, train_stats)\n    return window_data(df_local_norm, win_cfg)\n\n\ndef ui_predict_from_csv(file_obj, window_index: int, use_llm: bool):\n    if file_obj is None:\n        return None, \"Please upload a CSV file.\", \"\"\n    key = file_obj.name\n    if key not in UI_CACHE:\n        Xw, yw = load_csv_windows(key)\n        UI_CACHE[key] = (Xw, yw)\n    Xw, _ = UI_CACHE[key]\n    if len(Xw) == 0:\n        return None, \"No windows generated from file.\", \"\"\n    idx = int(np.clip(window_index, 0, len(Xw) - 1))\n    window = Xw[idx]\n    pred_class, confidence = predict_window(window)\n    explanation = run_explain(window, use_llm=use_llm)\n    fig = build_plot(window)\n    return fig, f\"Predicted: {pred_class} ({FAULT_KB[pred_class]['name']}) | confidence={confidence:.2f}\", explanation\n\n\ndef parse_manual_csv(text: str) -> np.ndarray:\n    from io import StringIO\n    df_manual = pd.read_csv(StringIO(text))\n    missing = [c for c in FEATURE_COLS if c not in df_manual.columns]\n    if missing:\n        raise ValueError(f\"Missing columns in manual CSV: {missing}\")\n    df_manual = df_manual[FEATURE_COLS]\n    if len(df_manual) < win_cfg.window_size:\n        raise ValueError(f\"Need at least {win_cfg.window_size} rows for one window\")\n    df_manual = normalize(df_manual, train_stats)\n    window = df_manual.values[: win_cfg.window_size].astype(np.float32)\n    return window\n\n\ndef ui_predict_manual(text: str, use_llm: bool):\n    try:\n        window = parse_manual_csv(text)\n    except Exception as exc:\n        return None, f\"Error: {exc}\", \"\"\n    pred_class, confidence = predict_window(window)\n    explanation = run_explain(window, use_llm=use_llm)\n    fig = build_plot(window)\n    return fig, f\"Predicted: {pred_class} ({FAULT_KB[pred_class]['name']}) | confidence={confidence:.2f}\", explanation\n\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# FDD Demo: LLM-Assisted Fault Explanation\")\n    gr.Markdown(\"Upload a CSV or paste rows to get fault predictions and explanations.\")\n\n    with gr.Row():\n        use_llm = gr.Checkbox(value=True, label=\"Use LLM for explanation (fallback if unavailable)\")\n\n    with gr.Tab(\"Upload CSV\"):\n        file_in = gr.File(file_types=[\".csv\"], label=\"Upload CSV\")\n        window_index = gr.Slider(0, 100, value=0, step=1, label=\"Window index\")\n        btn = gr.Button(\"Predict\")\n        plot_out = gr.Plot()\n        pred_out = gr.Textbox(label=\"Prediction\")\n        exp_out = gr.Textbox(label=\"Explanation\", lines=8)\n\n        btn.click(ui_predict_from_csv, inputs=[file_in, window_index, use_llm], outputs=[plot_out, pred_out, exp_out])\n\n    with gr.Tab(\"Manual CSV Window\"):\n        gr.Markdown(\"Paste CSV with columns: \" + \", \".join(FEATURE_COLS))\n        manual_text = gr.Textbox(lines=10, label=\"CSV Rows\")\n        btn2 = gr.Button(\"Predict\")\n        plot_out2 = gr.Plot()\n        pred_out2 = gr.Textbox(label=\"Prediction\")\n        exp_out2 = gr.Textbox(label=\"Explanation\", lines=8)\n\n        btn2.click(ui_predict_manual, inputs=[manual_text, use_llm], outputs=[plot_out2, pred_out2, exp_out2])\n\n\n# Launch\ndemo.launch(share=False)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}